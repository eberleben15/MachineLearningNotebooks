{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ben Eberle Term Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The Framingham Heart Study is a long-term, ongoing cardiovascular cohort study of residents of the city of Framingham, Massachusetts. The study began in 1948 with 5,209 adult subjects from Framingham, and is now on its third generation of participants. The dataset that I am using has 16 attributes per participant including gender, age, education, number of cigerettes per day, cholesterol, blood pressure and more. My goals for this projects are as follows:<br>\n",
    "1. Build Neural Network with Optimizers\n",
    "2. Partition Framingham Heart Disease Dataset into Training and Testing sets\n",
    "3. Classify Patients by 10YearCHD\n",
    "4. Predict Glucose Level using Regression\n",
    "\n",
    "Link to notebook - https://www.kaggle.com/amanajmera1/framingham-heart-study-dataset\n",
    "<br>In this notebook I have used code from CS445 notebook 7.2, A2, A3 and A4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetwork Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optimizers\n",
    "import sys  # for sys.float_info.epsilon\n",
    "\n",
    "######################################################################\n",
    "## class NeuralNetwork()\n",
    "######################################################################\n",
    "\n",
    "class NeuralNetwork():\n",
    "\n",
    "\n",
    "    def __init__(self, n_inputs, n_hiddens_per_layer, n_outputs, activation_function='tanh'):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "        # Set self.n_hiddens_per_layer to [] if argument is 0, [], or [0]\n",
    "        if n_hiddens_per_layer == 0 or n_hiddens_per_layer == [] or n_hiddens_per_layer == [0]:\n",
    "            self.n_hiddens_per_layer = []\n",
    "        else:\n",
    "            self.n_hiddens_per_layer = n_hiddens_per_layer\n",
    "\n",
    "        # Initialize weights, by first building list of all weight matrix shapes.\n",
    "        n_in = n_inputs\n",
    "        shapes = []\n",
    "        for nh in self.n_hiddens_per_layer:\n",
    "            shapes.append((n_in + 1, nh))\n",
    "            n_in = nh\n",
    "        shapes.append((n_in + 1, n_outputs))\n",
    "\n",
    "        # self.all_weights:  vector of all weights\n",
    "        # self.Ws: list of weight matrices by layer\n",
    "        self.all_weights, self.Ws = self.make_weights_and_views(shapes)\n",
    "\n",
    "        # Define arrays to hold gradient values.\n",
    "        # One array for each W array with same shape.\n",
    "        self.all_gradients, self.dE_dWs = self.make_weights_and_views(shapes)\n",
    "\n",
    "        self.trained = False\n",
    "        self.total_epochs = 0\n",
    "        self.error_trace = []\n",
    "        self.Xmeans = None\n",
    "        self.Xstds = None\n",
    "        self.Tmeans = None\n",
    "        self.Tstds = None\n",
    "\n",
    "\n",
    "    def make_weights_and_views(self, shapes):\n",
    "        # vector of all weights built by horizontally stacking flatenned matrices\n",
    "        # for each layer initialized with uniformly-distributed values.\n",
    "        all_weights = np.hstack([np.random.uniform(size=shape).flat / np.sqrt(shape[0])\n",
    "                                 for shape in shapes])\n",
    "        # Build list of views by reshaping corresponding elements from vector of all weights\n",
    "        # into correct shape for each layer.\n",
    "        views = []\n",
    "        start = 0\n",
    "        for shape in shapes:\n",
    "            size =shape[0] * shape[1]\n",
    "            views.append(all_weights[start:start + size].reshape(shape))\n",
    "            start += size\n",
    "        return all_weights, views\n",
    "\n",
    "\n",
    "    # Return string that shows how the constructor was called\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}({self.n_inputs}, {self.n_hiddens_per_layer}, {self.n_outputs}, \\'{self.activation_function}\\')'\n",
    "\n",
    "\n",
    "    # Return string that is more informative to the user about the state of this neural network.\n",
    "    def __str__(self):\n",
    "        result = self.__repr__()\n",
    "        if len(self.error_trace) > 0:\n",
    "            return self.__repr__() + f' trained for {len(self.error_trace)} epochs, final training error {self.error_trace[-1]:.4f}'\n",
    "\n",
    "\n",
    "    def train(self, X, T, n_epochs, learning_rate, method='sgd', verbose=True):\n",
    "        '''\n",
    "train: \n",
    "  X: n_samples x n_inputs matrix of input samples, one per row\n",
    "  T: n_samples x n_outputs matrix of target output values, one sample per row\n",
    "  n_epochs: number of passes to take through all samples updating weights each pass\n",
    "  learning_rate: factor controlling the step size of each update\n",
    "  method: is either 'sgd' or 'adam'\n",
    "        '''\n",
    "\n",
    "        # Setup standardization parameters\n",
    "        if self.Xmeans is None:\n",
    "            self.Xmeans = X.mean(axis=0)\n",
    "            self.Xstds = X.std(axis=0)\n",
    "            self.Xstds[self.Xstds == 0] = 1  # So we don't divide by zero when standardizing\n",
    "            self.Tmeans = T.mean(axis=0)\n",
    "            self.Tstds = T.std(axis=0)\n",
    "            \n",
    "        # Standardize X and T\n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "        T = (T - self.Tmeans) / self.Tstds\n",
    "\n",
    "        # Instantiate Optimizers object by giving it vector of all weights\n",
    "        optimizer = optimizers.Optimizers(self.all_weights)\n",
    "\n",
    "        # Define function to convert value from error_f into error in original T units, \n",
    "        # but only if the network has a single output. Multiplying by self.Tstds for \n",
    "        # multiple outputs does not correctly unstandardize the error.\n",
    "        if len(self.Tstds) == 1:\n",
    "            error_convert_f = lambda err: (np.sqrt(err) * self.Tstds)[0] # to scalar\n",
    "        else:\n",
    "            error_convert_f = lambda err: np.sqrt(err)[0] # to scalar\n",
    "            \n",
    "\n",
    "        if method == 'sgd':\n",
    "\n",
    "            error_trace = optimizer.sgd(self.error_f, self.gradient_f,\n",
    "                                        fargs=[X, T], n_epochs=n_epochs,\n",
    "                                        learning_rate=learning_rate,\n",
    "                                        verbose=True,\n",
    "                                        error_convert_f=error_convert_f)\n",
    "\n",
    "        elif method == 'adam':\n",
    "\n",
    "            error_trace = optimizer.adam(self.error_f, self.gradient_f,\n",
    "                                         fargs=[X, T], n_epochs=n_epochs,\n",
    "                                         learning_rate=learning_rate,\n",
    "                                         verbose=True,\n",
    "                                         error_convert_f=error_convert_f)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"method must be 'sgd' or 'adam'\")\n",
    "        \n",
    "        self.error_trace = error_trace\n",
    "\n",
    "        # Return neural network object to allow applying other methods after training.\n",
    "        #  Example:    Y = nnet.train(X, T, 100, 0.01).use(X)\n",
    "        return self\n",
    "\n",
    "    def relu(self, s):\n",
    "        s[s < 0] = 0\n",
    "        return s\n",
    "\n",
    "    def grad_relu(self, s):\n",
    "        return (s > 0).astype(int)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        '''X assumed already standardized. Output returned as standardized.'''\n",
    "        self.Ys = [X]\n",
    "        for W in self.Ws[:-1]:\n",
    "            if self.activation_function == 'relu':\n",
    "                self.Ys.append(self.relu(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
    "            else:\n",
    "                self.Ys.append(np.tanh(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
    "        last_W = self.Ws[-1]\n",
    "        self.Ys.append(self.Ys[-1] @ last_W[1:, :] + last_W[0:1, :])\n",
    "        return self.Ys\n",
    "\n",
    "    # Function to be minimized by optimizer method, mean squared error\n",
    "    def error_f(self, X, T):\n",
    "        Ys = self.forward_pass(X)\n",
    "        mean_sq_error = np.mean((T - Ys[-1]) ** 2)\n",
    "        return mean_sq_error\n",
    "\n",
    "    # Gradient of function to be minimized for use by optimizer method\n",
    "    def gradient_f(self, X, T):\n",
    "        '''Assumes forward_pass just called with layer outputs in self.Ys.'''\n",
    "        error = T - self.Ys[-1]\n",
    "        n_samples = X.shape[0]\n",
    "        n_outputs = T.shape[1]\n",
    "        delta = - error / (n_samples * n_outputs)\n",
    "        n_layers = len(self.n_hiddens_per_layer) + 1\n",
    "        # Step backwards through the layers to back-propagate the error (delta)\n",
    "        for layeri in range(n_layers - 1, -1, -1):\n",
    "            # gradient of all but bias weights\n",
    "            self.dE_dWs[layeri][1:, :] = self.Ys[layeri].T @ delta\n",
    "            # gradient of just the bias weights\n",
    "            self.dE_dWs[layeri][0:1, :] = np.sum(delta, 0)\n",
    "            # Back-propagate this layer's delta to previous layer\n",
    "            if self.activation_function == 'relu':\n",
    "                delta = delta @ self.Ws[layeri][1:, :].T * self.grad_relu(self.Ys[layeri])\n",
    "            else:\n",
    "                delta = delta @ self.Ws[layeri][1:, :].T * (1 - self.Ys[layeri] ** 2)\n",
    "        return self.all_gradients\n",
    "\n",
    "    def use(self, X):\n",
    "        '''X assumed to not be standardized'''\n",
    "        # Standardize X\n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "        Ys = self.forward_pass(X)\n",
    "        Y = Ys[-1]\n",
    "        # Unstandardize output Y before returning it\n",
    "        return Y * self.Tstds + self.Tmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_k_fold_cross_validation_sets(X, T, n_folds, shuffle=True):\n",
    "\n",
    "    if shuffle:\n",
    "        # Randomly order X and T\n",
    "        randorder = np.arange(X.shape[0])\n",
    "        np.random.shuffle(randorder)\n",
    "        X = X[randorder, :]\n",
    "        T = T[randorder, :]\n",
    "\n",
    "    # Partition X and T into folds\n",
    "    n_samples = X.shape[0]\n",
    "    n_per_fold = round(n_samples / n_folds)\n",
    "    n_last_fold = n_samples - n_per_fold * (n_folds - 1)\n",
    "\n",
    "    folds = []\n",
    "    start = 0\n",
    "    for foldi in range(n_folds-1):\n",
    "        folds.append( (X[start:start + n_per_fold, :], T[start:start + n_per_fold, :]) )\n",
    "        start += n_per_fold\n",
    "    folds.append( (X[start:, :], T[start:, :]) )\n",
    "\n",
    "    # Yield k(k-1) assignments of Xtrain, Train, Xvalidate, Tvalidate, Xtest, Ttest\n",
    "\n",
    "    for validation_i in range(n_folds):\n",
    "        for test_i in range(n_folds):\n",
    "            if test_i == validation_i:\n",
    "                continue\n",
    "\n",
    "            train_i = np.setdiff1d(range(n_folds), [validation_i, test_i])\n",
    "\n",
    "            Xvalidate, Tvalidate = folds[validation_i]\n",
    "            Xtest, Ttest = folds[test_i]\n",
    "            if len(train_i) > 1:\n",
    "                Xtrain = np.vstack([folds[i][0] for i in train_i])\n",
    "                Ttrain = np.vstack([folds[i][1] for i in train_i])\n",
    "            else:\n",
    "                Xtrain, Ttrain = folds[train_i[0]]\n",
    "\n",
    "            yield Xtrain, Ttrain, Xvalidate, Tvalidate, Xtest, Ttest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "The classification portion of my project is focused on classifiying patents by the 10yearCHD attribute which is an indicator of a participants 10 year risk of Coronary Heart Disease. The 10yearCHD attribute is presented as a binary value, 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkClassifier(NeuralNetwork):\n",
    "        \n",
    "    \n",
    "    def train(self, X, T, n_epochs, learning_rate, method='sgd', verbose=True):\n",
    "        '''\n",
    "train: \n",
    "  X: n_samples x n_inputs matrix of input samples, one per row\n",
    "  T: n_samples x n_outputs matrix of target output values, one sample per row\n",
    "  n_epochs: number of passes to take through all samples updating weights each pass\n",
    "  learning_rate: factor controlling the step size of each update\n",
    "  method: is either 'sgd' or 'adam'\n",
    "        '''\n",
    "        T = self.makeIndicatorVars(T)\n",
    "\n",
    "        # Setup standardization parameters\n",
    "        if self.Xmeans is None:\n",
    "            self.Xmeans = X.mean(axis=0)\n",
    "            self.Xstds = X.std(axis=0)\n",
    "            self.Xstds[self.Xstds == 0] = 1  # So we don't divide by zero when standardizing\n",
    "            self.Tmeans = T.mean(axis=0)\n",
    "            self.Tstds = T.std(axis=0)\n",
    "            \n",
    "        # Standardize X and T\n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "\n",
    "        # Instantiate Optimizers object by giving it vector of all weights\n",
    "        optimizer = optimizers.Optimizers(self.all_weights)\n",
    "\n",
    "        # Define function to convert value from error_f into error in original T units, \n",
    "        # but only if the network has a single output. Multiplying by self.Tstds for \n",
    "        # multiple outputs does not correctly unstandardize the error.\n",
    "        if len(self.Tstds) == 1:\n",
    "            error_convert_f = lambda err: (np.sqrt(err) * self.Tstds)[0] # to scalar\n",
    "        else:\n",
    "            error_convert_f = lambda err: np.sqrt(err) # to scalar\n",
    "            \n",
    "\n",
    "        if method == 'sgd':\n",
    "\n",
    "            error_trace = optimizer.sgd(self.error_f, self.gradient_f,\n",
    "                                        fargs=[X, T], n_epochs=n_epochs,\n",
    "                                        learning_rate=learning_rate,\n",
    "                                        verbose=True,\n",
    "                                        error_convert_f=error_convert_f)\n",
    "\n",
    "        elif method == 'adam':\n",
    "\n",
    "            error_trace = optimizer.adam(self.error_f, self.gradient_f,\n",
    "                                         fargs=[X, T], n_epochs=n_epochs,\n",
    "                                         learning_rate=learning_rate,\n",
    "                                         verbose=True,\n",
    "                                         error_convert_f=error_convert_f)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"method must be 'sgd' or 'adam'\")\n",
    "        \n",
    "        self.error_trace = error_trace\n",
    "\n",
    "        # Return neural network object to allow applying other methods after training.\n",
    "        #  Example:    Y = nnet.train(X, T, 100, 0.01).use(X)\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    # Function to be minimized by optimizer method, mean squared error\n",
    "    def error_f(self, X, T):\n",
    "        Ys = self.forward_pass(X)\n",
    "        Ys = self.softmax(Ys[-1])\n",
    "        return np.exp(np.mean(T * np.log(Ys)))\n",
    "    \n",
    "        \n",
    "    # Gradient of function to be minimized for use by optimizer method\n",
    "    def gradient_f(self, X, T):\n",
    "        '''Assumes forward_pass just called with layer outputs in self.Ys.'''\n",
    "        error = T - self.softmax(self.Ys[-1])\n",
    "        n_samples = X.shape[0]\n",
    "        n_outputs = T.shape[1]\n",
    "        delta = - error / (n_samples * n_outputs)\n",
    "        n_layers = len(self.n_hiddens_per_layer) + 1\n",
    "        # Step backwards through the layers to back-propagate the error (delta)\n",
    "        for layeri in range(n_layers - 1, -1, -1):\n",
    "            # gradient of all but bias weights\n",
    "            self.dE_dWs[layeri][1:, :] = self.Ys[layeri].T @ delta\n",
    "            # gradient of just the bias weights\n",
    "            self.dE_dWs[layeri][0:1, :] = np.sum(delta, 0)\n",
    "            # Back-propagate this layer's delta to previous layer\n",
    "            if self.activation_function == 'relu':\n",
    "                delta = delta @ self.Ws[layeri][1:, :].T * self.grad_relu(self.Ys[layeri])\n",
    "            else:\n",
    "                delta = delta @ self.Ws[layeri][1:, :].T * (1 - self.Ys[layeri] ** 2)\n",
    "        return self.all_gradients\n",
    "    \n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        '''X assumed already standardized. Output returned as standardized.'''\n",
    "        self.Ys = [X]\n",
    "        for W in self.Ws[:-1]:\n",
    "            if self.activation_function == 'relu':\n",
    "                self.Ys.append(self.relu(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
    "            else:\n",
    "                self.Ys.append(np.tanh(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
    "        last_W = self.Ws[-1]\n",
    "        self.Ys.append(self.Ys[-1] @ last_W[1:, :] + last_W[0:1, :])\n",
    "        return self.Ys\n",
    "    \n",
    "    \n",
    "    def use(self, X):\n",
    "        '''X assumed to not be standardized'''\n",
    "        # Standardize X\n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "        Ys = self.forward_pass(X)\n",
    "        Y = Ys[-1]\n",
    "        logregOutput = self.softmax(Y)\n",
    "        predicted = np.argmax(logregOutput,axis=1)\n",
    "        classes = np.unique(T)[np.argmax(Y, axis=1)].reshape(-1, 1)\n",
    "        return classes, predicted\n",
    "    \n",
    "    \n",
    "    def softmax(self, X):\n",
    "        fs = np.exp(X)  # N x K\n",
    "        denom = np.sum(fs, axis=1).reshape((-1, 1))\n",
    "        gs = fs / denom\n",
    "        return gs\n",
    "\n",
    "\n",
    "    def makeIndicatorVars(self, T):\n",
    "        # Make sure T is two-dimensional. Should be nSamples x 1.\n",
    "        if T.ndim == 1:\n",
    "            T = T.reshape((-1, 1))    \n",
    "        return (T == np.unique(T)).astype(int)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4231</th>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>24.96</td>\n",
       "      <td>80.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4232</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>23.14</td>\n",
       "      <td>60.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4233</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>25.97</td>\n",
       "      <td>66.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>126.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>19.71</td>\n",
       "      <td>65.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>133.5</td>\n",
       "      <td>83.0</td>\n",
       "      <td>21.47</td>\n",
       "      <td>80.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3656 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      male  age  education  currentSmoker  cigsPerDay  BPMeds  \\\n",
       "0        1   39        4.0              0         0.0     0.0   \n",
       "1        0   46        2.0              0         0.0     0.0   \n",
       "2        1   48        1.0              1        20.0     0.0   \n",
       "3        0   61        3.0              1        30.0     0.0   \n",
       "4        0   46        3.0              1        23.0     0.0   \n",
       "...    ...  ...        ...            ...         ...     ...   \n",
       "4231     1   58        3.0              0         0.0     0.0   \n",
       "4232     1   68        1.0              0         0.0     0.0   \n",
       "4233     1   50        1.0              1         1.0     0.0   \n",
       "4234     1   51        3.0              1        43.0     0.0   \n",
       "4237     0   52        2.0              0         0.0     0.0   \n",
       "\n",
       "      prevalentStroke  prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  \\\n",
       "0                   0             0         0    195.0  106.0   70.0  26.97   \n",
       "1                   0             0         0    250.0  121.0   81.0  28.73   \n",
       "2                   0             0         0    245.0  127.5   80.0  25.34   \n",
       "3                   0             1         0    225.0  150.0   95.0  28.58   \n",
       "4                   0             0         0    285.0  130.0   84.0  23.10   \n",
       "...               ...           ...       ...      ...    ...    ...    ...   \n",
       "4231                0             1         0    187.0  141.0   81.0  24.96   \n",
       "4232                0             1         0    176.0  168.0   97.0  23.14   \n",
       "4233                0             1         0    313.0  179.0   92.0  25.97   \n",
       "4234                0             0         0    207.0  126.5   80.0  19.71   \n",
       "4237                0             0         0    269.0  133.5   83.0  21.47   \n",
       "\n",
       "      heartRate  glucose  TenYearCHD  \n",
       "0          80.0     77.0           0  \n",
       "1          95.0     76.0           0  \n",
       "2          75.0     70.0           0  \n",
       "3          65.0    103.0           1  \n",
       "4          85.0     85.0           0  \n",
       "...         ...      ...         ...  \n",
       "4231       80.0     81.0           0  \n",
       "4232       60.0     79.0           1  \n",
       "4233       66.0     86.0           1  \n",
       "4234       65.0     68.0           0  \n",
       "4237       80.0    107.0           0  \n",
       "\n",
       "[3656 rows x 16 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "df = pandas.read_csv('framingham_heart_disease.csv', na_values='NA')\n",
    "df = df.dropna()\n",
    "X = df.iloc[:, :-1].values\n",
    "T = df.iloc[:, -1:].values\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSES\n",
      "[0 1]\n",
      "Adam: Epoch 5 Error=0.86438\n",
      "Adam: Epoch 10 Error=0.87322\n",
      "Adam: Epoch 15 Error=0.88317\n",
      "Adam: Epoch 20 Error=0.89310\n",
      "Adam: Epoch 25 Error=0.90122\n",
      "Adam: Epoch 30 Error=0.90670\n",
      "Adam: Epoch 35 Error=0.90913\n",
      "Adam: Epoch 40 Error=0.90964\n",
      "Adam: Epoch 45 Error=0.90963\n",
      "Adam: Epoch 50 Error=0.90985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Likelihood')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hiddens = [10]\n",
    "print('CLASSES')\n",
    "print(np.unique(T))\n",
    "nnet = NeuralNetworkClassifier(X.shape[1], hiddens, len(np.unique(T)))\n",
    "nnet.train(X, T, 50, 0.01, method='adam', verbose=True)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(nnet.error_trace)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Likelihood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(1219, 15) (1219, 1) (1218, 15) (1218, 1) (1219, 15) (1219, 1)\n",
      "Adam: Epoch 1000 Error=0.99242\n",
      "Adam: Epoch 2000 Error=0.99770\n",
      "Adam: Epoch 3000 Error=0.99899\n",
      "Adam: Epoch 4000 Error=0.99938\n",
      "Adam: Epoch 5000 Error=0.99972\n",
      "Adam: Epoch 6000 Error=0.99988\n",
      "Adam: Epoch 7000 Error=0.99994\n",
      "Adam: Epoch 8000 Error=0.99996\n",
      "Adam: Epoch 9000 Error=0.99998\n",
      "Adam: Epoch 10000 Error=0.99999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNetworkClassifier(15, [20], 2, 'tanh')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for Xtrain, Ttrain, Xvalidate, Tvalidate, Xtest, Ttest in generate_k_fold_cross_validation_sets(X, T, 3):\n",
    "    print()\n",
    "    #print('Xtrain\\n', Xtrain)\n",
    "    #print('Ttrain\\n', Ttrain)\n",
    "    #print('Xvalidate\\n', Xvalidate)\n",
    "    #print('Tvalidate\\n', Tvalidate)\n",
    "    #print('Xtest\\n', Xtest)\n",
    "    #print('Ttest\\n', Ttest)\n",
    "print(Xtrain.shape, Ttrain.shape,  Xvalidate.shape, Tvalidate.shape,  Xtest.shape, Ttest.shape)\n",
    "\n",
    "classes = np.arange(2)\n",
    "(Ttrain == classes).shape\n",
    "\n",
    "n_epochs = 10000\n",
    "learning_rate = 0.01\n",
    "\n",
    "np.random.seed(142)\n",
    "\n",
    "nnet = NeuralNetworkClassifier(Xtrain.shape[1], [20], len(classes))\n",
    "nnet.train(Xtrain, Ttrain, n_epochs, learning_rate, method='adam', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetworkClassifier(15, [20], 2, 'tanh') trained for 10000 epochs, final training error 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(nnet)\n",
    "plt.plot(nnet.error_trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Observations\n",
    "As we can see in the graph above our model did not need very many epochs to begin effectivly classifying participants into the 10yearCHD category. If we train the network for only 50 epochs we can see that around 25 epochs the likelyhood of predicting a patents 10yearCHD begins to level off and grow steadily from there. This is suprising to me but could be an indicator of a strong correlation between the chosen attributes for the study and an obvious side effect of the dataset I have chosen. \"Of course the attributes are strongly correlated, years of research have shown that these attributes are important and doctors/researchers use them for studying coronary heart disease.\" If we increase the size of the network and the number of epochs we can get the network to classify with 100% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NonLinear Regression\n",
    "For the regression portion of my project, I am going to use my model to predict participant glucose level. I have removed the 10yeardCHD attribute so this network will be trained using only 14 attributes. Below I will be running an experiment similar to the one run in notebook 7.2 \"Optimizers, Data Partitioning, Finding Good Parameters\" in which we compare the performance of different network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.   39.    4.   ... 70.   26.97 80.  ]\n",
      " [ 0.   46.    2.   ... 81.   28.73 95.  ]\n",
      " [ 1.   48.    1.   ... 80.   25.34 75.  ]\n",
      " ...\n",
      " [ 1.   50.    1.   ... 92.   25.97 66.  ]\n",
      " [ 1.   51.    3.   ... 80.   19.71 65.  ]\n",
      " [ 0.   52.    2.   ... 83.   21.47 80.  ]]\n",
      "[[ 77.]\n",
      " [ 76.]\n",
      " [ 70.]\n",
      " ...\n",
      " [ 86.]\n",
      " [ 68.]\n",
      " [107.]]\n",
      "(3656, 14) (3656, 1)\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:, :-2].values     #Partition data for new experiment\n",
    "T = df.iloc[:, -2:-1].values\n",
    "print(X)\n",
    "print(T)\n",
    "print(X.shape, T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Xtrain\n",
      " [[  1.    51.     1.   ... 104.    34.97  90.  ]\n",
      " [  1.    38.     4.   ...  74.    21.19  65.  ]\n",
      " [  0.    42.     3.   ...  91.5   27.78  95.  ]\n",
      " ...\n",
      " [  0.    47.     4.   ...  61.    20.32  84.  ]\n",
      " [  1.    51.     4.   ...  80.    19.36  60.  ]\n",
      " [  0.    44.     1.   ...  68.    32.82  88.  ]]\n",
      "Ttrain\n",
      " [[ 65.]\n",
      " [ 89.]\n",
      " [ 74.]\n",
      " ...\n",
      " [110.]\n",
      " [ 66.]\n",
      " [ 80.]]\n",
      "Xvalidate\n",
      " [[  0.    52.     2.   ...  89.    25.37 115.  ]\n",
      " [  0.    43.     2.   ...  81.    21.85  70.  ]\n",
      " [  0.    63.     1.   ...  81.    31.71  64.  ]\n",
      " ...\n",
      " [  1.    47.     4.   ...  84.    19.14  68.  ]\n",
      " [  0.    51.     1.   ...  79.    26.91  84.  ]\n",
      " [  0.    62.     1.   ...  75.    22.91  58.  ]]\n",
      "Tvalidate\n",
      " [[84.]\n",
      " [72.]\n",
      " [80.]\n",
      " ...\n",
      " [74.]\n",
      " [76.]\n",
      " [80.]]\n",
      "Xtest\n",
      " [[ 0.   63.    1.   ... 82.   25.58 78.  ]\n",
      " [ 0.   40.    2.   ... 72.   29.62 85.  ]\n",
      " [ 0.   41.    1.   ... 93.   35.42 68.  ]\n",
      " ...\n",
      " [ 0.   61.    1.   ... 76.   31.13 71.  ]\n",
      " [ 1.   57.    3.   ... 80.   26.56 75.  ]\n",
      " [ 1.   50.    4.   ... 78.   26.26 79.  ]]\n",
      "Ttest\n",
      " [[73.]\n",
      " [70.]\n",
      " [87.]\n",
      " ...\n",
      " [85.]\n",
      " [60.]\n",
      " [83.]]\n",
      "\n",
      "Xtrain\n",
      " [[ 0.   63.    1.   ... 82.   25.58 78.  ]\n",
      " [ 0.   40.    2.   ... 72.   29.62 85.  ]\n",
      " [ 0.   41.    1.   ... 93.   35.42 68.  ]\n",
      " ...\n",
      " [ 0.   61.    1.   ... 76.   31.13 71.  ]\n",
      " [ 1.   57.    3.   ... 80.   26.56 75.  ]\n",
      " [ 1.   50.    4.   ... 78.   26.26 79.  ]]\n",
      "Ttrain\n",
      " [[73.]\n",
      " [70.]\n",
      " [87.]\n",
      " ...\n",
      " [85.]\n",
      " [60.]\n",
      " [83.]]\n",
      "Xvalidate\n",
      " [[  0.    52.     2.   ...  89.    25.37 115.  ]\n",
      " [  0.    43.     2.   ...  81.    21.85  70.  ]\n",
      " [  0.    63.     1.   ...  81.    31.71  64.  ]\n",
      " ...\n",
      " [  1.    47.     4.   ...  84.    19.14  68.  ]\n",
      " [  0.    51.     1.   ...  79.    26.91  84.  ]\n",
      " [  0.    62.     1.   ...  75.    22.91  58.  ]]\n",
      "Tvalidate\n",
      " [[84.]\n",
      " [72.]\n",
      " [80.]\n",
      " ...\n",
      " [74.]\n",
      " [76.]\n",
      " [80.]]\n",
      "Xtest\n",
      " [[  1.    51.     1.   ... 104.    34.97  90.  ]\n",
      " [  1.    38.     4.   ...  74.    21.19  65.  ]\n",
      " [  0.    42.     3.   ...  91.5   27.78  95.  ]\n",
      " ...\n",
      " [  0.    47.     4.   ...  61.    20.32  84.  ]\n",
      " [  1.    51.     4.   ...  80.    19.36  60.  ]\n",
      " [  0.    44.     1.   ...  68.    32.82  88.  ]]\n",
      "Ttest\n",
      " [[ 65.]\n",
      " [ 89.]\n",
      " [ 74.]\n",
      " ...\n",
      " [110.]\n",
      " [ 66.]\n",
      " [ 80.]]\n",
      "\n",
      "Xtrain\n",
      " [[  1.    51.     1.   ... 104.    34.97  90.  ]\n",
      " [  1.    38.     4.   ...  74.    21.19  65.  ]\n",
      " [  0.    42.     3.   ...  91.5   27.78  95.  ]\n",
      " ...\n",
      " [  0.    47.     4.   ...  61.    20.32  84.  ]\n",
      " [  1.    51.     4.   ...  80.    19.36  60.  ]\n",
      " [  0.    44.     1.   ...  68.    32.82  88.  ]]\n",
      "Ttrain\n",
      " [[ 65.]\n",
      " [ 89.]\n",
      " [ 74.]\n",
      " ...\n",
      " [110.]\n",
      " [ 66.]\n",
      " [ 80.]]\n",
      "Xvalidate\n",
      " [[ 0.   63.    1.   ... 82.   25.58 78.  ]\n",
      " [ 0.   40.    2.   ... 72.   29.62 85.  ]\n",
      " [ 0.   41.    1.   ... 93.   35.42 68.  ]\n",
      " ...\n",
      " [ 0.   61.    1.   ... 76.   31.13 71.  ]\n",
      " [ 1.   57.    3.   ... 80.   26.56 75.  ]\n",
      " [ 1.   50.    4.   ... 78.   26.26 79.  ]]\n",
      "Tvalidate\n",
      " [[73.]\n",
      " [70.]\n",
      " [87.]\n",
      " ...\n",
      " [85.]\n",
      " [60.]\n",
      " [83.]]\n",
      "Xtest\n",
      " [[  0.    52.     2.   ...  89.    25.37 115.  ]\n",
      " [  0.    43.     2.   ...  81.    21.85  70.  ]\n",
      " [  0.    63.     1.   ...  81.    31.71  64.  ]\n",
      " ...\n",
      " [  1.    47.     4.   ...  84.    19.14  68.  ]\n",
      " [  0.    51.     1.   ...  79.    26.91  84.  ]\n",
      " [  0.    62.     1.   ...  75.    22.91  58.  ]]\n",
      "Ttest\n",
      " [[84.]\n",
      " [72.]\n",
      " [80.]\n",
      " ...\n",
      " [74.]\n",
      " [76.]\n",
      " [80.]]\n",
      "\n",
      "Xtrain\n",
      " [[  0.    52.     2.   ...  89.    25.37 115.  ]\n",
      " [  0.    43.     2.   ...  81.    21.85  70.  ]\n",
      " [  0.    63.     1.   ...  81.    31.71  64.  ]\n",
      " ...\n",
      " [  1.    47.     4.   ...  84.    19.14  68.  ]\n",
      " [  0.    51.     1.   ...  79.    26.91  84.  ]\n",
      " [  0.    62.     1.   ...  75.    22.91  58.  ]]\n",
      "Ttrain\n",
      " [[84.]\n",
      " [72.]\n",
      " [80.]\n",
      " ...\n",
      " [74.]\n",
      " [76.]\n",
      " [80.]]\n",
      "Xvalidate\n",
      " [[ 0.   63.    1.   ... 82.   25.58 78.  ]\n",
      " [ 0.   40.    2.   ... 72.   29.62 85.  ]\n",
      " [ 0.   41.    1.   ... 93.   35.42 68.  ]\n",
      " ...\n",
      " [ 0.   61.    1.   ... 76.   31.13 71.  ]\n",
      " [ 1.   57.    3.   ... 80.   26.56 75.  ]\n",
      " [ 1.   50.    4.   ... 78.   26.26 79.  ]]\n",
      "Tvalidate\n",
      " [[73.]\n",
      " [70.]\n",
      " [87.]\n",
      " ...\n",
      " [85.]\n",
      " [60.]\n",
      " [83.]]\n",
      "Xtest\n",
      " [[  1.    51.     1.   ... 104.    34.97  90.  ]\n",
      " [  1.    38.     4.   ...  74.    21.19  65.  ]\n",
      " [  0.    42.     3.   ...  91.5   27.78  95.  ]\n",
      " ...\n",
      " [  0.    47.     4.   ...  61.    20.32  84.  ]\n",
      " [  1.    51.     4.   ...  80.    19.36  60.  ]\n",
      " [  0.    44.     1.   ...  68.    32.82  88.  ]]\n",
      "Ttest\n",
      " [[ 65.]\n",
      " [ 89.]\n",
      " [ 74.]\n",
      " ...\n",
      " [110.]\n",
      " [ 66.]\n",
      " [ 80.]]\n",
      "\n",
      "Xtrain\n",
      " [[ 0.   63.    1.   ... 82.   25.58 78.  ]\n",
      " [ 0.   40.    2.   ... 72.   29.62 85.  ]\n",
      " [ 0.   41.    1.   ... 93.   35.42 68.  ]\n",
      " ...\n",
      " [ 0.   61.    1.   ... 76.   31.13 71.  ]\n",
      " [ 1.   57.    3.   ... 80.   26.56 75.  ]\n",
      " [ 1.   50.    4.   ... 78.   26.26 79.  ]]\n",
      "Ttrain\n",
      " [[73.]\n",
      " [70.]\n",
      " [87.]\n",
      " ...\n",
      " [85.]\n",
      " [60.]\n",
      " [83.]]\n",
      "Xvalidate\n",
      " [[  1.    51.     1.   ... 104.    34.97  90.  ]\n",
      " [  1.    38.     4.   ...  74.    21.19  65.  ]\n",
      " [  0.    42.     3.   ...  91.5   27.78  95.  ]\n",
      " ...\n",
      " [  0.    47.     4.   ...  61.    20.32  84.  ]\n",
      " [  1.    51.     4.   ...  80.    19.36  60.  ]\n",
      " [  0.    44.     1.   ...  68.    32.82  88.  ]]\n",
      "Tvalidate\n",
      " [[ 65.]\n",
      " [ 89.]\n",
      " [ 74.]\n",
      " ...\n",
      " [110.]\n",
      " [ 66.]\n",
      " [ 80.]]\n",
      "Xtest\n",
      " [[  0.    52.     2.   ...  89.    25.37 115.  ]\n",
      " [  0.    43.     2.   ...  81.    21.85  70.  ]\n",
      " [  0.    63.     1.   ...  81.    31.71  64.  ]\n",
      " ...\n",
      " [  1.    47.     4.   ...  84.    19.14  68.  ]\n",
      " [  0.    51.     1.   ...  79.    26.91  84.  ]\n",
      " [  0.    62.     1.   ...  75.    22.91  58.  ]]\n",
      "Ttest\n",
      " [[84.]\n",
      " [72.]\n",
      " [80.]\n",
      " ...\n",
      " [74.]\n",
      " [76.]\n",
      " [80.]]\n",
      "\n",
      "Xtrain\n",
      " [[  0.    52.     2.   ...  89.    25.37 115.  ]\n",
      " [  0.    43.     2.   ...  81.    21.85  70.  ]\n",
      " [  0.    63.     1.   ...  81.    31.71  64.  ]\n",
      " ...\n",
      " [  1.    47.     4.   ...  84.    19.14  68.  ]\n",
      " [  0.    51.     1.   ...  79.    26.91  84.  ]\n",
      " [  0.    62.     1.   ...  75.    22.91  58.  ]]\n",
      "Ttrain\n",
      " [[84.]\n",
      " [72.]\n",
      " [80.]\n",
      " ...\n",
      " [74.]\n",
      " [76.]\n",
      " [80.]]\n",
      "Xvalidate\n",
      " [[  1.    51.     1.   ... 104.    34.97  90.  ]\n",
      " [  1.    38.     4.   ...  74.    21.19  65.  ]\n",
      " [  0.    42.     3.   ...  91.5   27.78  95.  ]\n",
      " ...\n",
      " [  0.    47.     4.   ...  61.    20.32  84.  ]\n",
      " [  1.    51.     4.   ...  80.    19.36  60.  ]\n",
      " [  0.    44.     1.   ...  68.    32.82  88.  ]]\n",
      "Tvalidate\n",
      " [[ 65.]\n",
      " [ 89.]\n",
      " [ 74.]\n",
      " ...\n",
      " [110.]\n",
      " [ 66.]\n",
      " [ 80.]]\n",
      "Xtest\n",
      " [[ 0.   63.    1.   ... 82.   25.58 78.  ]\n",
      " [ 0.   40.    2.   ... 72.   29.62 85.  ]\n",
      " [ 0.   41.    1.   ... 93.   35.42 68.  ]\n",
      " ...\n",
      " [ 0.   61.    1.   ... 76.   31.13 71.  ]\n",
      " [ 1.   57.    3.   ... 80.   26.56 75.  ]\n",
      " [ 1.   50.    4.   ... 78.   26.26 79.  ]]\n",
      "Ttest\n",
      " [[73.]\n",
      " [70.]\n",
      " [87.]\n",
      " ...\n",
      " [85.]\n",
      " [60.]\n",
      " [83.]]\n",
      "(1219, 14) (1219, 1) (1218, 14) (1218, 1) (1219, 14) (1219, 1)\n"
     ]
    }
   ],
   "source": [
    "#Generate new sets\n",
    "for Xtrain, Ttrain, Xvalidate, Tvalidate, Xtest, Ttest in generate_k_fold_cross_validation_sets(X, T, 3):\n",
    "    print()\n",
    "    print('Xtrain\\n', Xtrain)\n",
    "    print('Ttrain\\n', Ttrain)\n",
    "    print('Xvalidate\\n', Xvalidate)\n",
    "    print('Tvalidate\\n', Tvalidate)\n",
    "    print('Xtest\\n', Xtest)\n",
    "    print('Ttest\\n', Ttest)\n",
    "print(Xtrain.shape, Ttrain.shape,  Xvalidate.shape, Tvalidate.shape,  Xtest.shape, Ttest.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam: Epoch 100 Error=19.29649\n",
      "Adam: Epoch 200 Error=19.29625\n",
      "Adam: Epoch 300 Error=19.29625\n",
      "Adam: Epoch 400 Error=19.29625\n",
      "Adam: Epoch 500 Error=19.29625\n",
      "Adam: Epoch 600 Error=19.29625\n",
      "Adam: Epoch 700 Error=19.29625\n",
      "Adam: Epoch 800 Error=19.29625\n",
      "Adam: Epoch 900 Error=19.29625\n",
      "Adam: Epoch 1000 Error=19.29625\n",
      "sgd: Epoch 100 Error=20.40215\n",
      "sgd: Epoch 200 Error=19.49970\n",
      "sgd: Epoch 300 Error=19.34609\n",
      "sgd: Epoch 400 Error=19.31343\n",
      "sgd: Epoch 500 Error=19.30407\n",
      "sgd: Epoch 600 Error=19.30049\n",
      "sgd: Epoch 700 Error=19.29880\n",
      "sgd: Epoch 800 Error=19.29787\n",
      "sgd: Epoch 900 Error=19.29732\n",
      "sgd: Epoch 1000 Error=19.29697\n",
      "   rate  epochs   nh method  train RMSE  validate RMSE  test RMSE\n",
      "0  0.01    1000  [0]   adam   19.296252      19.024252  18.111886\n",
      "1  0.01    1000  [0]    sgd   19.296969      19.025493  18.126445\n",
      "Adam: Epoch 100 Error=18.74709\n",
      "Adam: Epoch 200 Error=16.97804\n",
      "Adam: Epoch 300 Error=15.64756\n",
      "Adam: Epoch 400 Error=14.68312\n",
      "Adam: Epoch 500 Error=13.22485\n",
      "Adam: Epoch 600 Error=12.65735\n",
      "Adam: Epoch 700 Error=12.29548\n",
      "Adam: Epoch 800 Error=11.99294\n",
      "Adam: Epoch 900 Error=11.80687\n",
      "Adam: Epoch 1000 Error=11.70061\n",
      "sgd: Epoch 100 Error=23.28834\n",
      "sgd: Epoch 200 Error=22.98866\n",
      "sgd: Epoch 300 Error=22.70245\n",
      "sgd: Epoch 400 Error=22.35221\n",
      "sgd: Epoch 500 Error=21.94417\n",
      "sgd: Epoch 600 Error=21.52036\n",
      "sgd: Epoch 700 Error=21.12878\n",
      "sgd: Epoch 800 Error=20.78149\n",
      "sgd: Epoch 900 Error=20.47466\n",
      "sgd: Epoch 1000 Error=20.20550\n",
      "   rate  epochs    nh method  train RMSE  validate RMSE  test RMSE\n",
      "0  0.01    1000   [0]   adam   19.296252      19.024252  18.111886\n",
      "1  0.01    1000   [0]    sgd   19.296969      19.025493  18.126445\n",
      "2  0.01    1000  [10]   adam   11.699690      23.756664  25.853672\n",
      "3  0.01    1000  [10]    sgd   20.202998      21.108759  19.115170\n",
      "Adam: Epoch 100 Error=19.47617\n",
      "Adam: Epoch 200 Error=18.03600\n",
      "Adam: Epoch 300 Error=17.25750\n",
      "Adam: Epoch 400 Error=13.01183\n",
      "Adam: Epoch 500 Error=9.30429\n",
      "Adam: Epoch 600 Error=7.76354\n",
      "Adam: Epoch 700 Error=6.80763\n",
      "Adam: Epoch 800 Error=6.19133\n",
      "Adam: Epoch 900 Error=5.72658\n",
      "Adam: Epoch 1000 Error=5.36372\n",
      "sgd: Epoch 100 Error=23.63098\n",
      "sgd: Epoch 200 Error=23.51386\n",
      "sgd: Epoch 300 Error=23.49203\n",
      "sgd: Epoch 400 Error=23.48233\n",
      "sgd: Epoch 500 Error=23.47390\n",
      "sgd: Epoch 600 Error=23.46528\n",
      "sgd: Epoch 700 Error=23.45623\n",
      "sgd: Epoch 800 Error=23.44667\n",
      "sgd: Epoch 900 Error=23.43652\n",
      "sgd: Epoch 1000 Error=23.42570\n",
      "   rate  epochs        nh method  train RMSE  validate RMSE  test RMSE\n",
      "0  0.01    1000       [0]   adam   19.296252      19.024252  18.111886\n",
      "1  0.01    1000       [0]    sgd   19.296969      19.025493  18.126445\n",
      "2  0.01    1000      [10]   adam   11.699690      23.756664  25.853672\n",
      "3  0.01    1000      [10]    sgd   20.202998      21.108759  19.115170\n",
      "4  0.01    1000  [50, 10]   adam    5.371537      31.818162  34.192847\n",
      "5  0.01    1000  [50, 10]    sgd   23.425592      25.173844  21.847034\n",
      "Adam: Epoch 500 Error=19.29625\n",
      "Adam: Epoch 1000 Error=19.29625\n",
      "Adam: Epoch 1500 Error=19.29625\n",
      "Adam: Epoch 2000 Error=19.29625\n",
      "Adam: Epoch 2500 Error=19.29625\n",
      "Adam: Epoch 3000 Error=19.29625\n",
      "Adam: Epoch 3500 Error=19.29625\n",
      "Adam: Epoch 4000 Error=19.29625\n",
      "Adam: Epoch 4500 Error=19.29625\n",
      "Adam: Epoch 5000 Error=19.29625\n",
      "sgd: Epoch 500 Error=19.29766\n",
      "sgd: Epoch 1000 Error=19.29636\n",
      "sgd: Epoch 1500 Error=19.29627\n",
      "sgd: Epoch 2000 Error=19.29626\n",
      "sgd: Epoch 2500 Error=19.29625\n",
      "sgd: Epoch 3000 Error=19.29625\n",
      "sgd: Epoch 3500 Error=19.29625\n",
      "sgd: Epoch 4000 Error=19.29625\n",
      "sgd: Epoch 4500 Error=19.29625\n",
      "sgd: Epoch 5000 Error=19.29625\n",
      "   rate  epochs        nh method  train RMSE  validate RMSE  test RMSE\n",
      "0  0.01    1000       [0]   adam   19.296252      19.024252  18.111886\n",
      "1  0.01    1000       [0]    sgd   19.296969      19.025493  18.126445\n",
      "2  0.01    1000      [10]   adam   11.699690      23.756664  25.853672\n",
      "3  0.01    1000      [10]    sgd   20.202998      21.108759  19.115170\n",
      "4  0.01    1000  [50, 10]   adam    5.371537      31.818162  34.192847\n",
      "5  0.01    1000  [50, 10]    sgd   23.425592      25.173844  21.847034\n",
      "6  0.01    5000       [0]   adam   19.296252      19.024251  18.111888\n",
      "7  0.01    5000       [0]    sgd   19.296252      19.024254  18.111898\n",
      "Adam: Epoch 500 Error=12.98997\n",
      "Adam: Epoch 1000 Error=11.87934\n",
      "Adam: Epoch 1500 Error=11.68210\n",
      "Adam: Epoch 2000 Error=11.44480\n",
      "Adam: Epoch 2500 Error=11.25932\n",
      "Adam: Epoch 3000 Error=11.16144\n",
      "Adam: Epoch 3500 Error=11.10366\n",
      "Adam: Epoch 4000 Error=11.06296\n",
      "Adam: Epoch 4500 Error=11.02669\n",
      "Adam: Epoch 5000 Error=10.99038\n",
      "sgd: Epoch 500 Error=21.15583\n",
      "sgd: Epoch 1000 Error=19.39044\n",
      "sgd: Epoch 1500 Error=19.05991\n",
      "sgd: Epoch 2000 Error=18.95882\n",
      "sgd: Epoch 2500 Error=18.88682\n",
      "sgd: Epoch 3000 Error=18.82304\n",
      "sgd: Epoch 3500 Error=18.75500\n",
      "sgd: Epoch 4000 Error=18.66375\n",
      "sgd: Epoch 4500 Error=18.56091\n",
      "sgd: Epoch 5000 Error=18.47008\n",
      "   rate  epochs        nh method  train RMSE  validate RMSE  test RMSE\n",
      "0  0.01    1000       [0]   adam   19.296252      19.024252  18.111886\n",
      "1  0.01    1000       [0]    sgd   19.296969      19.025493  18.126445\n",
      "2  0.01    1000      [10]   adam   11.699690      23.756664  25.853672\n",
      "3  0.01    1000      [10]    sgd   20.202998      21.108759  19.115170\n",
      "4  0.01    1000  [50, 10]   adam    5.371537      31.818162  34.192847\n",
      "5  0.01    1000  [50, 10]    sgd   23.425592      25.173844  21.847034\n",
      "6  0.01    5000       [0]   adam   19.296252      19.024251  18.111888\n",
      "7  0.01    5000       [0]    sgd   19.296252      19.024254  18.111898\n",
      "8  0.01    5000      [10]   adam   10.990313      24.266768  24.321886\n",
      "9  0.01    5000      [10]    sgd   18.469912      20.397406  19.170103\n",
      "Adam: Epoch 500 Error=7.73931\n",
      "Adam: Epoch 1000 Error=4.79981\n",
      "Adam: Epoch 1500 Error=4.01779\n",
      "Adam: Epoch 2000 Error=3.55112\n",
      "Adam: Epoch 2500 Error=3.22816\n",
      "Adam: Epoch 3000 Error=2.99598\n",
      "Adam: Epoch 3500 Error=3.43106\n",
      "Adam: Epoch 4000 Error=3.33459\n",
      "Adam: Epoch 4500 Error=2.18002\n",
      "Adam: Epoch 5000 Error=2.08102\n",
      "sgd: Epoch 500 Error=23.47451\n",
      "sgd: Epoch 1000 Error=23.39907\n",
      "sgd: Epoch 1500 Error=23.25701\n",
      "sgd: Epoch 2000 Error=22.70148\n",
      "sgd: Epoch 2500 Error=21.05464\n",
      "sgd: Epoch 3000 Error=19.95398\n",
      "sgd: Epoch 3500 Error=19.22445\n",
      "sgd: Epoch 4000 Error=19.02126\n",
      "sgd: Epoch 4500 Error=18.92504\n",
      "sgd: Epoch 5000 Error=18.85550\n",
      "    rate  epochs        nh method  train RMSE  validate RMSE  test RMSE\n",
      "0   0.01    1000       [0]   adam   19.296252      19.024252  18.111886\n",
      "1   0.01    1000       [0]    sgd   19.296969      19.025493  18.126445\n",
      "2   0.01    1000      [10]   adam   11.699690      23.756664  25.853672\n",
      "3   0.01    1000      [10]    sgd   20.202998      21.108759  19.115170\n",
      "4   0.01    1000  [50, 10]   adam    5.371537      31.818162  34.192847\n",
      "5   0.01    1000  [50, 10]    sgd   23.425592      25.173844  21.847034\n",
      "6   0.01    5000       [0]   adam   19.296252      19.024251  18.111888\n",
      "7   0.01    5000       [0]    sgd   19.296252      19.024254  18.111898\n",
      "8   0.01    5000      [10]   adam   10.990313      24.266768  24.321886\n",
      "9   0.01    5000      [10]    sgd   18.469912      20.397406  19.170103\n",
      "10  0.01    5000  [50, 10]   adam    2.064155      39.932349  38.896897\n",
      "11  0.01    5000  [50, 10]    sgd   18.855377      20.292776  19.020036\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for rate in [0.01]: # learning rates\n",
    "    for epochs in [1000, 5000]: # Training durations\n",
    "        for nh in [[0], [10], [50, 10]]: # Layer sizes\n",
    "            for method in ['adam', 'sgd']: # Optimizers\n",
    "                # instantiate and train\n",
    "                nnet = NeuralNetwork(Xtrain.shape[1], nh, Ttrain.shape[1])\n",
    "                nnet.train(Xtrain, Ttrain, epochs, rate, method=method)\n",
    "                # append the results of each experiment\n",
    "                results.append([rate, epochs, nh, method,\n",
    "                                rmse(Ttrain, nnet.use(Xtrain)),\n",
    "                                rmse(Tvalidate, nnet.use(Xvalidate)),\n",
    "                                rmse(Ttest, nnet.use(Xtest))])\n",
    "                # load these into a dataframe and give it some column titles\n",
    "                df2 = pandas.DataFrame(results, columns=('rate', 'epochs', 'nh', 'method', 'train RMSE',\n",
    "                                                    'validate RMSE', 'test RMSE'))\n",
    "            print(df2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NonLinear Regression Observations\n",
    "My results from the regression experiments are very interesting. As we can see above, the network is not very good at predicting a participants glucose level but shows promissing results as we increase the size of the network. We can also see that in this case adam outperforms sgd by a lot, probably because of the obvious performance advantges of adam. I believe that if I were to train the network for much longer, the overall performance would increase to an acceptable level, however for these smaller network sizes the performace seems to be unacceptable and unreliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Framingham Heart Disease Dataset - https://www.kaggle.com/amanajmera1/framingham-heart-study-dataset\n",
    "2. CS445 Notebook 7.2\n",
    "3. CS445 A2\n",
    "4. CS445 A3\n",
    "5. CS445 A4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
