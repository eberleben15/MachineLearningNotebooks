{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A5 1-D and 2-D Convolutional Neural Networks in Pytorch\n",
    "\n",
    "* A5.4: *Changed structure of `CNN2D.__init__` by having it call `make_cnn_and_fc_layers` function. It is this function that `CNN1D` must override., not the `__init__` constructor.*\n",
    "* A5.3: *Added two missing statements in `CNN2D` that initialize `layeri` to 0 and increment it by 1*\n",
    "* A5.2: *added an exception to `CNN2D.__init__` code that provides a helpful message if you specify an impossible configuration for convolutional layers.  Repeat this exception code in your `CNN1D.__init__` function.*\n",
    "* A5.1: *small edit in CNN2D to allow empty list for `n_hiddens_per_fc_layer`.*\n",
    "\n",
    "In this assignment, you will experiment with the given convolutional neural network for 2-dimensional input samples (images), in class `CNN2D`, by applying it to the MNIST data.  You will also define a new class for handling 1-dimensional input samples, called `CNN1D`, that extends `CNN2D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `CNN2D` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a definition of `CNN2D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class CNN2D(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, n_outputs,\n",
    "                 patch_size_per_conv_layer, stride_per_conv_layer, activation_function='tanh', device='cpu'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        n_conv_layers = len(n_hiddens_per_conv_layer)\n",
    "        if (len(patch_size_per_conv_layer) != n_conv_layers\n",
    "            or len(stride_per_conv_layer) != n_conv_layers):\n",
    "            raise Exception('The lengths of n_hiddens_per_conv_layer, patch_size_per_conv_layer, and stride_per_conv_layer must be equal.')\n",
    "\n",
    "        self.activation_function = torch.tanh if activation_function == 'tanh' else torch.relu\n",
    "\n",
    "        self.make_conv_and_fc_layers(n_inputs, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, n_outputs,\n",
    "                                     patch_size_per_conv_layer, stride_per_conv_layer)\n",
    "        \n",
    "        self.Xmeans = None\n",
    "        self.to(self.device)\n",
    "\n",
    "    def make_conv_and_fc_layers(self, n_inputs, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, n_outputs,\n",
    "                                patch_size_per_conv_layer, stride_per_conv_layer):\n",
    "                # Create all convolutional layers\n",
    "        # First argument to first Conv2d is number of channels for each pixel.\n",
    "        # Just 1 for our grayscale images.\n",
    "        n_in = 1\n",
    "        input_hw = int(np.sqrt(n_inputs))  # original input image height (=width because image assumed square)\n",
    "        self.conv_layers = torch.nn.ModuleList()\n",
    "        layeri = 0\n",
    "        for nh, patch_size, stride in zip(n_hiddens_per_conv_layer,\n",
    "                                          patch_size_per_conv_layer,\n",
    "                                          stride_per_conv_layer):\n",
    "            self.conv_layers.append(torch.nn.Conv2d(n_in, nh, kernel_size=patch_size, stride=stride))\n",
    "            conv_layer_output_hw = (input_hw - patch_size) // stride + 1\n",
    "            if conv_layer_output_hw <= 0:\n",
    "                raise Exception(f'''For conv layer {layeri}, input_hw of {input_hw} is less than patch_size {patch_size}.\n",
    "Try reducing the patch_size for this layer or for the previous layer.''')\n",
    "            input_hw = conv_layer_output_hw  # for next trip through this loop\n",
    "            n_in = nh\n",
    "            layeri += 1\n",
    "           \n",
    "        # Create all fully connected layers.  First must determine number of inputs to first\n",
    "        # fully-connected layer that results from flattening the images coming out of the last\n",
    "        # convolutional layer.\n",
    "        n_in = input_hw ** 2 * n_in\n",
    "        self.fc_layers = torch.nn.ModuleList()\n",
    "        for nh in n_hiddens_per_fc_layer:\n",
    "            self.fc_layers.append(torch.nn.Linear(n_in, nh))\n",
    "            n_in = nh\n",
    "        self.fc_layers.append(torch.nn.Linear(n_in, n_outputs))\n",
    "\n",
    "    def forward_all_outputs(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        Ys = [X]\n",
    "        for conv_layer in self.conv_layers:\n",
    "            Ys.append(self.activation_function(conv_layer(Ys[-1])))\n",
    "\n",
    "        flattened_input = Ys[-1].reshape(n_samples, -1)\n",
    "\n",
    "        for layeri, fc_layer in enumerate(self.fc_layers[:-1]):\n",
    "            if layeri == 0:\n",
    "                Ys.append(self.activation_function(fc_layer(flattened_input)))\n",
    "            else:\n",
    "                Ys.append(self.activation_function(fc_layer(Ys[-1])))\n",
    "\n",
    "        if len(self.fc_layers) == 1:\n",
    "            # only the output layer\n",
    "            Ys.append(self.fc_layers[-1](flattened_input))\n",
    "        else:\n",
    "            Ys.append(self.fc_layers[-1](Ys[-1]))\n",
    "\n",
    "        return Ys\n",
    "\n",
    "    def forward(self, X):\n",
    "        Ys = self.forward_all_outputs(X)\n",
    "        return Ys[-1]\n",
    "\n",
    "    def train(self, X, T, batch_size, n_epochs, learning_rate, method='sgd', verbose=True):\n",
    "        '''X and T must be numpy arrays'''\n",
    "\n",
    "        self.classes = np.unique(T)\n",
    "        T = np.arange(len(self.classes))[np.where(T.reshape(-1, 1) == self.classes)[1]]\n",
    "\n",
    "        # Set data matrices to torch.tensors\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        T = torch.from_numpy(T).long().to(self.device)  # required for classification in pytorch\n",
    "\n",
    "        # Setup standardization parameters\n",
    "        if self.Xmeans is None:\n",
    "            self.Xmeans = X.mean(axis=0)\n",
    "            self.Xstds = X.std(axis=0)\n",
    "            self.Xstds[self.Xstds == 0] = 1  # So we don't divide by zero when standardizing\n",
    "\n",
    "        # Standardize X\n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "\n",
    "        X.requires_grad_(True)\n",
    "\n",
    "        if method == 'sgd':\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        CELoss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.error_trace = []\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            num_batches = X.shape[0] // batch_size\n",
    "            loss_sum = 0\n",
    "\n",
    "            for k in range(num_batches):\n",
    "                start = k * batch_size\n",
    "                end = (k + 1) * batch_size\n",
    "                X_batch = X[start:end, ...]\n",
    "                T_batch = T[start:end, ...]\n",
    "\n",
    "                Y = self.forward(X_batch)\n",
    "\n",
    "                loss = CELoss(Y, T_batch)\n",
    "                loss.backward()\n",
    "\n",
    "                # Update parameters\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss_sum += loss\n",
    "\n",
    "            self.error_trace.append(loss_sum / num_batches)\n",
    "\n",
    "            if verbose and (epoch + 1) % (max(1, n_epochs // 10)) == 0:\n",
    "                print(f'{method}: Epoch {epoch + 1} Loss {self.error_trace[-1]:.3f}')\n",
    "\n",
    "        return self\n",
    "\n",
    "    def softmax(self, Y):\n",
    "        '''Apply to final layer weighted sum outputs'''\n",
    "        # Trick to avoid overflow\n",
    "        maxY = torch.max(Y, axis=1)[0].reshape((-1, 1))\n",
    "        expY = torch.exp(Y - maxY)\n",
    "        denom = torch.sum(expY, axis=1).reshape((-1, 1))\n",
    "        Y = expY / denom\n",
    "        return Y\n",
    "\n",
    "    def use(self, X):\n",
    "        # Set input matrix to torch.tensors\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        # Standardize X\n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "        # Calculate output of net for all samples in X\n",
    "        Y = self.forward(X)\n",
    "        # Convert output to class probabilities\n",
    "        probs = self.softmax(Y)\n",
    "        # For each sample pick highest probability and translate that to class labels\n",
    "        classes = self.classes[torch.argmax(probs, axis=1).cpu().numpy()].reshape(-1, 1)\n",
    "        return classes, probs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN2D on MNIST Digits\n",
    "\n",
    "We will use a bunch (50,000) images of hand drawn digits from [this deeplearning.net site](http://deeplearning.net/tutorial/gettingstarted.html).  Download `mnist.pkl.gz` if you don't already have it from A4. \n",
    "\n",
    "This pickle file includes data already partitioned into training, validation, and test sets.  To read it into python, use the following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1, 28, 28) (50000, 1) (10000, 784) (10000, 1) (10000, 1, 28, 28) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "\n",
    "Xtrain = train_set[0]\n",
    "Ttrain = train_set[1].reshape(-1, 1)\n",
    "\n",
    "Xval = valid_set[0]\n",
    "Tval = valid_set[1].reshape(-1, 1)\n",
    "\n",
    "Xtest = test_set[0]\n",
    "Ttest = test_set[1].reshape(-1, 1)\n",
    "\n",
    "Xtrain = Xtrain.reshape(-1, 1, 28, 28)\n",
    "Xtest = Xtest.reshape(-1, 1, 28, 28)\n",
    "\n",
    "print(Xtrain.shape, Ttrain.shape,  Xval.shape, Tval.shape,  Xtest.shape, Ttest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    y_or_n = input('Would you like to run on the GPU? (y or n): ')\n",
    "    if y_or_n == 'y' or y_or_n == 'yes':\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(Ttrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [10, 1, 10, 10], but got 3-dimensional input of size [500, 1, 2000] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0f0a67a6697e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcnnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pytorch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-76fde65921a9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, T, batch_size, n_epochs, learning_rate, method, verbose)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mT_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-76fde65921a9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mYs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_all_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mYs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-76fde65921a9>\u001b[0m in \u001b[0;36mforward_all_outputs\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mYs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mYs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mflattened_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 395\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [10, 1, 10, 10], but got 3-dimensional input of size [500, 1, 2000] instead"
     ]
    }
   ],
   "source": [
    "n_hiddens_per_conv_layer = [10, 10]\n",
    "patch_size_per_conv_layer = [10, 5]\n",
    "stride_per_conv_layer=[4, 2]\n",
    "n_hiddens_per_fc_layer = [5]\n",
    "\n",
    "cnnet = CNN2D(28 * 28, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, len(np.unique(Ttrain)), \n",
    "              patch_size_per_conv_layer, stride_per_conv_layer, device=device)\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 500\n",
    "learning_rate = 0.01\n",
    "\n",
    "cnnet.train(Xtrain, Ttrain, batch_size, n_epochs, learning_rate, method='adam')\n",
    "\n",
    "plt.plot(cnnet.error_trace, label='Pytorch')\n",
    "plt.title('MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(Y_classes, T):\n",
    "    class_names = np.unique(T)\n",
    "    table = []\n",
    "    for true_class in class_names:\n",
    "        row = []\n",
    "        for Y_class in class_names:\n",
    "            row.append(100 * np.mean(Y_classes[T == true_class] == Y_class))\n",
    "        table.append(row)\n",
    "    conf_matrix = pandas.DataFrame(table, index=class_names, columns=class_names)\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classes, _ = cnnet.use(Xtest)\n",
    "perc_correct = 100 * np.mean(Classes == Ttest)\n",
    "print(f'Test accuracy in percent correct: {perc_correct:.2f}')\n",
    "confusion_matrix(Classes, Ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "To explore the effects of different CNN structures, show results for the following steps.  For each architecture, use the same number of epochs, batch size, and learning rate as used above.\n",
    "\n",
    "1. Compare test accuracy of CNN2D nets with one, two and three convolutional layers, each with 10 units and patch sizes of 5 and strides of 1.\n",
    "2. Using the best number of convolutional layers found in Step 1, compare the test accuracies of CNN2d nets with zero, one, and two fully-connected layers each with 10 hidden units.\n",
    "\n",
    "Combine the results of each of your runs and display them in a `pandas.Dataframe` that includes the network structure and percent correct on train and test sets. Discuss your results, and describe the network structure and training parameters that produced the best test results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN1D\n",
    "\n",
    "Complete the following code cell to define `CNN1D`.  The only change from `CNN2D` that is required is in the constructor.  Complete these steps.\n",
    "\n",
    "1. Copy the `make_conv_and_fc_layers` function from `CNN2D`.\n",
    "2. For each convolutional layer, create a `torch.nn.Conv1d` object instead of a `torch.nn.Conv2d` object.\n",
    "3. Modify the statement `input_hw = int(np.sqrt(n_inputs))` appropriately. `input_hw` refers to \"input height width\".  What would beed to change to make this make sense for a single dimensional sample?\n",
    "4. Modify the statement `n_in = input_hw ** 2 * n_in` appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1D(CNN2D):\n",
    "\n",
    "    def make_conv_and_fc_layers(self, n_inputs, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, n_outputs,\n",
    "                                patch_size_per_conv_layer, stride_per_conv_layer):\n",
    "        # Create all convolutional layers\n",
    "        # First argument to first Conv2d is number of channels for each pixel.\n",
    "        # Just 1 for our grayscale images.\n",
    "        n_in = 1\n",
    "        input_hw = int(n_inputs)  # original input image height (=width because image assumed square)\n",
    "        self.conv_layers = torch.nn.ModuleList()\n",
    "        layeri = 0\n",
    "        for nh, patch_size, stride in zip(n_hiddens_per_conv_layer,\n",
    "                                          patch_size_per_conv_layer,\n",
    "                                          stride_per_conv_layer):\n",
    "            self.conv_layers.append(torch.nn.Conv1d(n_in, nh, kernel_size=patch_size, stride=stride))\n",
    "            conv_layer_output_hw = (input_hw - patch_size) // stride + 1\n",
    "            if conv_layer_output_hw <= 0:\n",
    "                raise Exception(f'''For conv layer {layeri}, input_hw of {input_hw} is less than patch_size {patch_size}.\n",
    "Try reducing the patch_size for this layer or for the previous layer.''')\n",
    "            input_hw = conv_layer_output_hw  # for next trip through this loop\n",
    "            n_in = nh\n",
    "            layeri += 1\n",
    "           \n",
    "        # Create all fully connected layers.  First must determine number of inputs to first\n",
    "        # fully-connected layer that results from flattening the images coming out of the last\n",
    "        # convolutional layer.\n",
    "        n_in = input_hw * n_in\n",
    "        self.fc_layers = torch.nn.ModuleList()\n",
    "        for nh in n_hiddens_per_fc_layer:\n",
    "            self.fc_layers.append(torch.nn.Linear(n_in, nh))\n",
    "            n_in = nh\n",
    "        self.fc_layers.append(torch.nn.Linear(n_in, n_outputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Data to Test `CNN1D`\n",
    "\n",
    "Here is some toy data to test your `CNN1D` definition.  Each sample is now 1-dimensional.  Let's make vectors of two kinds, ones with square pulses and ones with triangular pulses, at random locations and random durations. Both kinds will be 100 values long, with zeros between the waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_square_pulse():\n",
    "    sample = np.zeros(100)\n",
    "    for i in range(np.random.randint(1, 5)):  # making from 1 to 4 pulses\n",
    "        start = np.random.randint(0, 80)\n",
    "        width = np.random.randint(5, 20)\n",
    "        sample[start:start + width] = 1\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_pulse = make_square_pulse()\n",
    "square_pulse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(square_pulse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_triangular_pulse():\n",
    "    sample = np.zeros(100)\n",
    "    for i in range(np.random.randint(1, 5)):  # making from 1 to 4 pulses\n",
    "        start = np.random.randint(0, 80)\n",
    "        width = np.random.randint(5, 20)\n",
    "        if width % 2 == 1:\n",
    "            width += 1  # if odd, make it even\n",
    "        sample[start:start + width // 2] = np.linspace(0, 1, width // 2)\n",
    "        sample[start + width // 2:start + width] = np.linspace(1, 0, width // 2)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangular_pulse = make_triangular_pulse()\n",
    "triangular_pulse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(triangular_pulse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_each = 500\n",
    "Xtrain = np.array([make_square_pulse() for i in range(n_each)] +\n",
    "                   [make_triangular_pulse() for i in range(n_each)])\n",
    "Ttrain = np.array(['square'] * n_each + ['triangular'] * n_each).reshape(-1, 1)\n",
    "n_each = 500\n",
    "Xtest = np.array([make_square_pulse() for i in range(n_each)] +\n",
    "                   [make_triangular_pulse() for i in range(n_each)])\n",
    "Ttest = np.array(['square'] * n_each + ['triangular'] * n_each).reshape(-1, 1)\n",
    "Xtrain.shape, Ttrain.shape, Xtest.shape, Ttest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.newaxis == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = Xtrain[:, None, :]\n",
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = Xtrain.reshape(Xtrain.shape[0], 1, -1)\n",
    "Xtest = Xtest.reshape(Xtest.shape[0], 1, -1)\n",
    "Xtrain.shape, Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnet1 = CNN1D(100, [10, 5], [5, 5], 2, [10, 4], [1, 2], device=device)\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "cnnet1.train(Xtrain, Ttrain, batch_size, n_epochs, learning_rate, method='adam')\n",
    "\n",
    "plt.plot(cnnet1.error_trace, label='Pytorch')\n",
    "plt.title('Pulses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classes, _ = cnnet1.use(Xtest)\n",
    "perc_correct = 100 * np.mean(Classes == Ttest)\n",
    "print(f'Test accuracy in percent correct: {perc_correct:.2f}')\n",
    "confusion_matrix(Classes, Ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = list(cnnet1.children())[0][0].weight.data.cpu()  # in case running on GPU\n",
    "plt.plot(W[:, 0, :].T);\n",
    "W[:, 0, :].T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments on ECG Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An [electrocardiogram](), or ECG, is a record in time of a voltage generated by the heart.  It can be used to diagnose abnormalities in the heart.\n",
    "\n",
    "Public datasets containing ECG traces are available, such as the [Non-Invasive Fetal ECG Arrhythmia Database](https://physionet.org/content/nifeadb/1.0.0/) site. The data files there are in a standard waveform-database (WFDB) format.  As is often the case for most standard data formats you run in to, a python package exists for reading this data, called [wfdb](https://github.com/MIT-LCP/wfdb-python) that you can install using [conda](https://anaconda.org/conda-forge/wfdb).\n",
    "\n",
    "This data set includes ECG from normal patients and from ones with arrythmias, with data file names like `ARR_01.dat` and `NR_01.dat`, respectively.  We have already downloaded these files, read them in using the `wfdb` package and collected them into segments of 2000 voltages.  The sample rate for this data is 1000 Hz, so 2000 voltages spans 2 seconds. Download this data set from [ecg.npy](https://www.cs.colostate.edu/~cs445/notebooks/ecg.npy) \n",
    "\n",
    "Now, our job for our `CNN1D` is to classify each 2000 sample segment into the classes `normal` or `arrythmia`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have downloaded `ecg.npy`, you can load it and plot a few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg = np.load('ecg.npy')\n",
    "arr = ecg['arrythmia']\n",
    "norm = ecg['normal']\n",
    "arr.shape, norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(arr[0])\n",
    "plt.legend(('Arrythmia',))\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(arr[100])\n",
    "plt.legend(('Arrythmia',))\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(norm[0])\n",
    "plt.legend(('Normal',))\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(norm[100])\n",
    "plt.legend(('Normal',));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's stack the `arr` and `norm` samples together, create class labels for each sample, randomly rearrange them, and divide into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((arr, norm))\n",
    "X = X.reshape(X.shape[0], 1, -1)\n",
    "T = np.hstack((['arr'] * arr.shape[0], ['nr'] * norm.shape[0])).reshape(-1, 1)\n",
    "n_samples = X.shape[0]\n",
    "rows = np.arange(n_samples)\n",
    "np.random.shuffle(rows)\n",
    "n_train = int(n_samples * 0.8)\n",
    "Xtrain = X[rows[:n_train], ...]\n",
    "Ttrain = T[rows[:n_train], ...]\n",
    "Xtest = X[rows[n_train:], ...]\n",
    "Ttest = T[rows[n_train:], ...]\n",
    "\n",
    "Xtrain.shape, Ttrain.shape, Xtest.shape, Ttest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ttrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, ready to train.  Create a `CNN1D` network with a statements like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1d = CNN1D(Xtrain.shape[-1], [5, 10], [10, 10], 2, [100, 20], [20, 5], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, experiment with at least ten different network structures, patch sizes and strides and compare them with the percent accuracy on test data.  Combine the results of each of your runs and display them in a `pandas.Dataframe` that includes the network structure and percent correct on train and test sets. Discuss your results, and describe the network structure and training parameters that produced the best test results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading and Check-In\n",
    "Download [A5grader.zip](https://www.cs.colostate.edu/~cs445/notebooks/A5grader.zip) and extract A5grader.py from it. Run the code in the following cell to demonstrate an example grading session. Remember to test your code with additional tests of your own design. Your notebook must be named as Lastname-A5.ipynb.  \n",
    "\n",
    "When ready, submit your notebook via the A5 link in our class Canvas web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================= Code Execution =======================\n",
      "\n",
      "Extracting python code from notebook named 'Eberle_A5.ipynb' and storing in notebookcode.py\n",
      "Removing all statements that are not function or class defs or import statements.\n",
      "\n",
      "Testing\n",
      "\n",
      "    xs = np.arange(100)\n",
      "    n_each = 500\n",
      "    n_samples = n_each * 2\n",
      "    X = np.array([np.sin(xs / 2) + np.random.normal(0, 1, size=100) for i in range(n_each)] +\n",
      "                 [np.sin(xs / 3) + np.random.normal(0, 1, size=100) for i in range(n_each)])\n",
      "    X = X[:, np.newaxis, :]\n",
      "    T = np.array([2] * n_each + [3] * n_each).reshape(-1, 1)\n",
      "    rows = np.arange(n_samples)\n",
      "    np.random.shuffle(rows)\n",
      "    X = X[rows, ...]\n",
      "    T = T[rows, ...]\n",
      "    n_train = int(n_samples * 0.8)\n",
      "    Xtrain = X[:n_train, ...]\n",
      "    Ttrain = T[:n_train, :]\n",
      "    Xtest = X[n_train:, ...]\n",
      "    Ttest = T[n_train:, :]\n",
      "\n",
      "    cnn1d = CNN1D(100, [5, 5], [3], 2, [10, 5], [1, 2])\n",
      "    cnn1d.train(Xtrain, Ttrain, 10, 20, 0.01, method='adam')\n",
      "\n",
      "    perc_train = 100 * np.mean(cnn1d.use(Xtrain)[0] == Ttrain)\n",
      "    perc_test = 100 * np.mean(cnn1d.use(Xtest)[0] == Ttest)\n",
      "\n",
      "adam: Epoch 2 Loss 0.017\n",
      "adam: Epoch 4 Loss 0.005\n",
      "adam: Epoch 6 Loss 0.003\n",
      "adam: Epoch 8 Loss 0.002\n",
      "adam: Epoch 10 Loss 0.001\n",
      "adam: Epoch 12 Loss 0.001\n",
      "adam: Epoch 14 Loss 0.001\n",
      "adam: Epoch 16 Loss 0.000\n",
      "adam: Epoch 18 Loss 0.000\n",
      "adam: Epoch 20 Loss 0.000\n",
      "\n",
      "--- 40/40 points.  perc_train is correctly 100%.\n",
      "\n",
      "--- 40/40 points.  perc_test is correctly 100%.\n",
      "\n",
      "======================================================================\n",
      "MachineLearning Execution Grade is 80 / 80\n",
      "======================================================================\n",
      "\n",
      " __ / 10 Based on results of experiments with MNIST data.\n",
      "\n",
      " __ / 10 Based on results of experiments with ECG data.\n",
      "\n",
      "======================================================================\n",
      "MachineLearning FINAL GRADE is  _  / 100\n",
      "======================================================================\n",
      "\n",
      "Extra Credit:\n",
      "\n",
      "Earn up to 3 extra credit points on this assignment by doing any or all of the following experiments.\n",
      "\n",
      "1. Compare your results on the MNIST data by using relu versus tanh activation functions. Show and \n",
      "   discuss the results.\n",
      "\n",
      "2. Compare your results on the MNIST data using adam versus sgd. Show and discuss the results.\n",
      "\n",
      "3. Download another image data set, apply your CNN2D class to this data and discuss the results.\n",
      "\n",
      "MachineLearning EXTRA CREDIT is 0 / 3\n"
     ]
    }
   ],
   "source": [
    "%run -i A5grader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "Earn up to 3 extra credit points on this assignment by doing any or all of the following experiments. \n",
    "\n",
    "1. Compare your results on the MNIST data by using `relu` versus `tanh` activation functions. Show and discuss the results.\n",
    "2. Compare your results on the MNIST data using `adam` versus `sgd`. Show and discuss the results.\n",
    "3. Download another image data set, apply your `CNN2D` class to this data and discuss the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
